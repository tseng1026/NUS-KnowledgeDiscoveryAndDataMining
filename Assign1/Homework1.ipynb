{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Assignment 1  - YuTingTseng (E0503474)\n",
    "This answer sheet contains your answer to questions in Section 1~4. Sentences in <span style=\"color:blue\">blue</span> indicates questions to be answered. \n",
    "\n",
    "For open questions, please organize your answer into \"points\" to ease our grading efforts. For example, an answer to \"What is the most popular item? Explain you answer and describe any assumptions you’ve made.\" should look like:\n",
    "\n",
    "> The most popular item ID is 12345. The reason is that:\n",
    "> - reason 1\n",
    "> - reason 2\n",
    "> \n",
    "> I've made the following assumptions:\n",
    "> - assumption 1\n",
    "> - assumption 2\n",
    "\n",
    "If you wish to type math equations in markdown, follow this link: https://jupyter-notebook.readthedocs.io/en/stable/examples/Notebook/Working%20With%20Markdown%20Cells.html#LaTeX-equations\n",
    "\n",
    "Remember to save this Jupyter notebook as __YourNameInLumiNUS_YourNUSNETID.ipynb__."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Import all libraries and load data here**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is a code cell\n",
    "# -- Import all libraries and load data in this cell --\n",
    "import numpy  as np\n",
    "import pandas as pd\n",
    "import time\n",
    "import joblib\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.cluster   import KMeans\n",
    "from efficient_apriori import apriori\n",
    "\n",
    "data1 = joblib.load(\"assignment1.data\")  # numpy.ndarray\n",
    "data2 = pd.read_csv(\"record.csv\")        # pandas.core.frame.DataFrame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Section 1 - Clustering and Initialization (20 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:blue\"> Consider the nine data points (A, B, C, D, E, F, G, H, I) in Fig. 1. Taking the points D, E, and F as the initial cluster centers, apply the K-Means algorithm on the data, with the number of clusters K = 3. At the end of each iteration, list the positions of the cluster centers, as well as the set of points belonging to each cluster. Do you think this clustering result is satisfactory? </span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The initial cluster centroids are the position of $D$, $E$, $F$.<br>\n",
    "```\n",
    "iter = 0: Centroids [(0,1), (0,0), (1,0)] Labels of Data [0, 1, 2, 0, 1, 2, 0, 1, 2]\n",
    "iter = 1: Centroids [(0,1), (0,0), (1,0)] Labels of Data [0, 1, 2, 0, 1, 2, 0, 1, 2]\n",
    "iter = 2: Centroids [(0,1), (0,0), (1,0)] Labels of Data [0, 1, 2, 0, 1, 2, 0, 1, 2]\n",
    "iter = 3: Centroids [(0,1), (0,0), (1,0)] Labels of Data [0, 1, 2, 0, 1, 2, 0, 1, 2]\n",
    "...\n",
    "# the clustering result remains the same after several iterations (converged)```\n",
    "\n",
    "\n",
    "I think the clustering result is <strong>not satisfactory</strong>. The reason is that:\n",
    "1. We request for <em>less intracluster distance</em> and <em>larger intercluster distance</em>. However, the situation above doesn't achieve the goal; $(A, B, C)$, $(D, E, F)$ and $(G, H, I)$ are actually a better clustering result."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:blue\"> Initialization is important for K-means. Consider the following heuristic method for selecting the initial cluster center positions: <br><br>\n",
    "Choose the first center $c_1$ as the point A.<br>\n",
    "For $k=2,...,K$, set $c_k = \\underset{x\\in X}{\\arg\\max}(\\underset{i=1,...,k-1}{\\min}\\Vert x - c_i \\Vert_2)$, where X is the set of data points.<br><br>\n",
    "Apply this heuristic to the data points in Fig. 1. Show the computed cluster centers for K = 3. Next, run the K-means algorithm with the obtained cluster centers. At the end of each iteration, list the positions of the cluster centers, as well as the set of points belonging to each cluster.\n",
    "</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$A$ is the first center $c_1$. We would choose the point further from $A$, the point $I$ as $c_2$. The last cluster centroid following the instructions would be $E$.<br>\n",
    "The initial cluster centroids are the position of $A$, $I$, $D$.<br>\n",
    "```\n",
    "iter = 0: Centroids [(-5,1), (5,-1), (0,0)] Labels of Data [0, 0, 0, 1, 1, 1, 2, 2, 2]\n",
    "iter = 1: Centroids [(-5,0), (5, 0), (0,0)] Labels of Data [0, 0, 0, 1, 1, 1, 2, 2, 2]\n",
    "iter = 2: Centroids [(-5,0), (5, 0), (0,0)] Labels of Data [0, 0, 0, 1, 1, 1, 2, 2, 2]\n",
    "iter = 3: Centroids [(-5,0), (5, 0), (0,0)] Labels of Data [0, 0, 0, 1, 1, 1, 2, 2, 2]\n",
    "...\n",
    "# the clustering result remains the same after several iterations (converged)```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Section 2 - Selecting the Number of Clusters (10 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:blue\"> Here, we will explore how to select the number of clusters. Using Python 3.6, load the attached data file 'assignment1.data' using the following commands:</span>\n",
    "    \n",
    "<span style=\"color:blue\">import joblib<br>X = joblib.load('assignment1.data')</span>\n",
    "\n",
    "<span style=\"color:blue\">This results in X, which is a 400 by 2 matrix, where each row is a single sample, in 2 dimensions. Apply K-means on these samples with K ranging from 1 to 10. Plot a figure, where the y-axis is the Within Cluster Sum of Squares (WCSS) after convergence, and the x-axis is K from 1 to 10: </span>\n",
    "\n",
    "<span style=\"color:blue\">$WCSS = \\sum_{k=1}^K \\sum_{x \\in C_i} \\Vert x - c_i \\Vert_2^2$</span>\n",
    "\n",
    "<span style=\"color:blue\">Select a value of K that you think is appropriate for clustering this dataset, and explain the reason. </span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "k =  1, wcss = 22800.675548638064356\n",
      "k =  2, wcss = 10437.716090329677172\n",
      "k =  3, wcss = 3937.528039558056662\n",
      "k =  4, wcss = 517.886091708018171\n",
      "k =  5, wcss = 463.503867042240643\n",
      "k =  6, wcss = 415.291068142373376\n",
      "k =  7, wcss = 372.013486291308482\n",
      "k =  8, wcss = 323.771281256676843\n",
      "k =  9, wcss = 298.107795891113085\n",
      "k = 10, wcss = 268.832731678196069\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# This is a code cell\n",
    "# -- Your code for running clustering and plotting the figure goes here.\n",
    "\n",
    "x, y = [], []\n",
    "for k in range(1, 11):\n",
    "    kmeans = KMeans(n_clusters = k, random_state = 7122).fit(data1)\n",
    "    labels = kmeans.labels_\n",
    "    center = kmeans.cluster_centers_\n",
    "\n",
    "    wcss = 0\n",
    "    for i in range(400):\n",
    "        wcss += np.sum((data1[i] - center[labels[i]]) ** 2)\n",
    "    print (\"k = {:2d}, wcss = {:.15f}\".format(k, wcss))\n",
    "    x.append(k)\n",
    "    y.append(wcss)\n",
    "\n",
    "plt.bar(x, y, .5)\n",
    "plt.xlabel(\"k clusters\")\n",
    "plt.ylabel(\"wcss\")\n",
    "plt.xticks(x)\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I think <strong>$K = 4$</strong>  would be the best choice, the following statements are my reasons.<br>\n",
    "1. If the cluster number is $1$, $wcss$ is about $74.54$, which is actually a large number, indicating that the intracluster distance is extremely large.\n",
    "2. With the number of clusters increases, $wcss$ seems to decrease, showing that the distance within the cluster is deducing.\n",
    "3. As it goes to $4$, $wcss$ drops dramtically comparing with the previous value and tend to converge. We can get the results good enough (within $K = 1 - 10$) and simultaneously reduce the computing resource consumption."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Section 3 - Data Cleaning and Exploration (20 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:blue\"> 1) Before continuing, let us examine the dataset for “dirty” records to do some data cleaning. Remove the records with negative values of the Quantity variable, and the records with NaN values of the CustomerID variable. Report how many records were removed in total. </span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I first add a column with index. Next, I find the list of the index of invalid records, including those with negative quantity and with NaN customerID. The total number of items in the delete list is 143985."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Delete 143985 records\n"
     ]
    }
   ],
   "source": [
    "# This is a code cell\n",
    "# -- Your code for removing dirty records goes here --\n",
    "data2[\"Index\"] = list(range(len(data2.index)))\n",
    "deleteQ = set(data2[\"Index\"][data2[\"Quantity\"] < 0])\n",
    "deleteC = set(data2[\"Index\"][data2[\"CustomerID\"].isnull()])\n",
    "delete  = list(set.union(deleteQ, deleteC))\n",
    "delete.sort()\n",
    "print (\"Delete {} records\".format(len(delete)))\n",
    "data2 = data2.drop(delete,  axis = 0)\n",
    "data2 = data2.drop(\"Index\", axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:blue\"> 2)~9) Please provide the answers to the questions listed in the table. </span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting date of the dataset? 2010-12-01 08:26:00\n",
      "Ending   date of the dataset? 2011-12-09 12:50:00\n",
      "\n",
      "\n",
      "Number of Customers?     4339\n",
      "Number of Transactions? 18536\n",
      "Number of Different Kind of Items?  3665\n",
      "\n",
      "\n",
      "Number of transactions customer ID 17850 has made?     34\n",
      "Which customer (ID) has made the most transactions? 12748\n",
      "What is the item ID (i.e. StockCode) of the best-seller? 23843\n"
     ]
    }
   ],
   "source": [
    "# This is a code cell\n",
    "# -- (Optional) Your code for filling in the table goes here. --\n",
    "print (\"Starting date of the dataset? {}\".format(min(data2[\"InvoiceDate\"])))\n",
    "print (\"Ending   date of the dataset? {}\".format(max(data2[\"InvoiceDate\"])))\n",
    "print (\"\\n\")\n",
    "print (\"Number of Customers?    {:5d}\".format(len(data2.groupby(\"CustomerID\"))))\n",
    "print (\"Number of Transactions? {:5d}\".format(len(data2.groupby(\"InvoiceNo\" ))))\n",
    "print (\"Number of Different Kind of Items? {:5d}\".format(len(data2.groupby(\"StockCode\"))))\n",
    "print (\"\\n\")\n",
    "print (\"Number of transactions customer ID 17850 has made?  {:5d}\" .format(data2.groupby(\"CustomerID\")[\"InvoiceNo\"].nunique().at[17850.0]))\n",
    "print (\"Which customer (ID) has made the most transactions? {:.0f}\".format(data2.groupby(\"CustomerID\")[\"InvoiceNo\"].nunique().idxmax()))\n",
    "print (\"What is the item ID (i.e. StockCode) of the best-seller? {}\".format(data2.groupby(\"StockCode\")[\"Quantity\"].sum().idxmax()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a markdown cell. Please fill in your answer for (2)~(9).\n",
    "\n",
    "| No. | Question                                                                                                   | Answer       |\n",
    "|-----|------------------------------------------------------------------------------------------------------------|--------------|\n",
    "| 2)  | Starting date of the dataset?                                                                              | 2010-12-01 |\n",
    "| 3)  | Ending date of the dataset?                                                                                | 2011-12-09 |\n",
    "| 4)  | Number of customers?                                                                                       | 4339 |\n",
    "| 5)  | Number of transactions?                                                                                    | 18536 |\n",
    "| 6)  | Number of different kind of items?                                                                         | 3665 |\n",
    "| 7)  | Number of transactions customer ID 17850 has made?                                                         | 34 |\n",
    "| 8)  | Which customer (ID) has made the most transactions?                                                        | 12748 |\n",
    "| 9)  | What is the item ID (i.e. StockCode) of the best-seller? We define \"best-seller\" as the item with the highest sales volume. | 23843 |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:blue\"> 10) Next, let us get some general understanding about the transactions. Please make a histogram of the number of unique items per transaction (as described below) and describe one insight that you can observe from the plot.  \n",
    "(Note: You can plot this histogram by running matplotlib.hist() with 200 bins on the sequence of values $n_1, …, n_N$, where $n_i$ is the number of unique items in transaction $i$. This produces a histogram with “number of unique items in transaction” in the x-axis, and “count” in the y-axis, i.e. each bar counts how many transactions fall into the corresponding bucket. It is sufficient to comment on the general shape of the curve and what it implies about the data; it’s fine if the insight does not seem especially interesting.)\n",
    "</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZQAAAEGCAYAAABCa2PoAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjAsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+17YcXAAAbfElEQVR4nO3df5RdZX3v8feHgKIZSKBMc0OIDWpMVwotvxawFFknVSFAa8ByKVyEBLGpXlBRukrUemGJ3KaKuEC4cAPEQEVwvIDGgGLMZUr1iiaBQBJ+lAhDJSsmFTAQtLTB7/1jPxN2Ts7MnJk8Z/Y5yee11lmz97Ofvfd3nnPmfOfZP56tiMDMzGxn7VF1AGZmtmtwQjEzsyycUMzMLAsnFDMzy8IJxczMstiz6gBaYfz48fH2t7+96jCG9MorrzB27NiqwxiS48zLcebVCXF2QowAK1eu/FVEdI90/V0yoUyYMIEVK1ZUHcaQent7qdVqVYcxJMeZl+PMqxPi7IQYASQ9uzPr+5CXmZll4YRiZmZZOKGYmVkWTihmZpaFE4qZmWXhhGJmZlk4oZiZWRZOKGZmloUTipmZZbFL3infb8q8e3Yo65t/SgWRmJnt+txDMTOzLJxQzMwsCycUMzPLwgnFzMyycEIxM7MsnFDMzCwLJxQzM8vCCcXMzLJwQjEzsyycUMzMLAsnFDMzy8IJxczMsnBCMTOzLJxQzMwsCycUMzPLwgnFzMyyaFlCkTRZ0v2SHpO0VtInUvllktZLWpVeJ5fW+bSkdZKelHRiqXxmKlsnaV6rYjYzs5Fr5RMbtwIXR8RDkvYBVkpampZ9JSKuLFeWNB04E/gj4EDgh5LekRZfB7wPeA5YLmlxRDzWwtjNzGyYWpZQImIDsCFNvyzpcWDSIKvMAu6IiFeBZyStA45Oy9ZFxNMAku5IdZ1QzMzaiCKi9TuRpgAPAIcAnwLmAC8BKyh6MS9KuhZ4MCK+nta5Gfhe2sTMiPhwKj8HOCYiLqzbx1xgLkB3d/eRPT09rF6/eYdYDp00LvevN2Jbtmyhq6ur6jCG5Djzcpx5dUKcnRAjwIwZM1ZGxFEjXb+Vh7wAkNQF3AlcFBEvSboeuByI9PPLwId2dj8RsQBYADBt2rSo1WrMmXfPDvX6zq7t7K6y6e3tpVarVR3GkBxnXo4zr06IsxNizKGlCUXSXhTJ5LaIuAsgIjaWlt8ILEmz64HJpdUPSmUMUm5mZm2ilVd5CbgZeDwiriqVTyxVOw1Yk6YXA2dKeqOkg4GpwM+A5cBUSQdLegPFifvFrYrbzMxGppU9lHcB5wCrJa1KZZ8BzpJ0GMUhrz7grwEiYq2kHoqT7VuBCyLiNQBJFwL3AWOAhRGxtoVxm5nZCLTyKq8fAWqw6N5B1rkCuKJB+b2DrWdmZtXznfJmZpaFE4qZmWXhhGJmZlk4oZiZWRZOKGZmloUTipmZZeGEYmZmWTihmJlZFk4oZmaWhROKmZll4YRiZmZZOKGYmVkWTihmZpaFE4qZmWXhhGJmZlk4oZiZWRZOKGZmloUTipmZZeGEYmZmWTihmJlZFk4oZmaWhROKmZll4YRiZmZZOKGYmVkWTihmZpaFE4qZmWXhhGJmZlk4oZiZWRYtSyiSJku6X9JjktZK+kQq31/SUklPpZ/7pXJJukbSOkmPSjqitK3Zqf5Tkma3KmYzMxu5VvZQtgIXR8R04FjgAknTgXnAsoiYCixL8wAnAVPTay5wPRQJCLgUOAY4Gri0PwmZmVn7aFlCiYgNEfFQmn4ZeByYBMwCbknVbgFOTdOzgFuj8CAwXtJE4ERgaUS8EBEvAkuBma2K28zMRkYR0fqdSFOAB4BDgH+NiPGpXMCLETFe0hJgfkT8KC1bBlwC1IC9I+ILqfxzwG8j4sq6fcyl6NnQ3d19ZE9PD6vXb94hlkMnjWvFrzgiW7Zsoaurq+owhuQ483KceXVCnJ0QI8CMGTNWRsRRI11/z5zBNCKpC7gTuCgiXipySCEiQlKWjBYRC4AFANOmTYtarcaceffsUK/v7FqO3WXR29tLrVarOowhOc68HGdenRBnJ8SYQ0uv8pK0F0UyuS0i7krFG9OhLNLPTal8PTC5tPpBqWygcjMzayOtvMpLwM3A4xFxVWnRYqD/Sq3ZwHdK5eemq72OBTZHxAbgPuAESfulk/EnpDIzM2sjrTzk9S7gHGC1pFWp7DPAfKBH0vnAs8AZadm9wMnAOuA3wHkAEfGCpMuB5ane5yPihRbGbWZmI9CyhJJOrmuAxe9pUD+ACwbY1kJgYb7ozMwsN98pb2ZmWTihmJlZFk4oZmaWRcvvQ2k3U+ruTembf0pFkZiZ7VrcQzEzsyycUMzMLAsnFDMzy8IJxczMsnBCMTOzLJxQzMwsCycUMzPLwgnFzMyy2O1ubKxXf6Mj+GZHM7ORcA/FzMyycEIxM7MsnFDMzCwLJxQzM8vCCcXMzLJwQjEzsyycUMzMLAsnFDMzy8IJxczMsnBCMTOzLJxQzMwsi6YSiqRlzZSZmdnua9DBISXtDbwZOEDSfoDSon2BSS2OzczMOshQow3/NXARcCCwktcTykvAtS2My8zMOsygCSUirgaulvSxiPjqKMVkZmYdqKnnoUTEVyW9E5hSXicibh1oHUkLgT8DNkXEIansMuCvgH9L1T4TEfemZZ8GzgdeAz4eEfel8pnA1cAY4KaImD+M369l6p+j4meomNnurqmEIukfgbcBqyi+8AECGDChAIsoDovV1/lKRFxZt/3pwJnAH1EcXvuhpHekxdcB7wOeA5ZLWhwRjzUTt5mZjZ5mn9h4FDA9IqLZDUfEA5KmNFl9FnBHRLwKPCNpHXB0WrYuIp4GkHRHquuEYmbWZpq9D2UN8F8y7fNCSY9KWpiuHIPiirFflOo8l8oGKjczszajZjodku4HDgN+BrzaXx4R7x9ivSnAktI5lAnArygOl10OTIyID0m6FngwIr6e6t0MfC9tZmZEfDiVnwMcExEXNtjXXGAuQHd395E9PT2sXr95yN+tkUMnjdtuvpnt1K/TjC1bttDV1TXs9Uab48zLcebVCXF2QowAM2bMWBkRR410/WYPeV020h2URcTG/mlJNwJL0ux6YHKp6kGpjEHK67e9AFgAMG3atKjVasypO3HerL6za9vNN7Od+nWa0dvbS602/PVGm+PMy3Hm1QlxdkKMOTR7ldc/5diZpIkRsSHNnkZxKA1gMfANSVdRnJSfStEbEjBV0sEUieRM4L/liMXMzPJq9iqvlykOUwG8AdgLeCUi9h1knduBGsVd9s8BlwI1SYelbfVR3DhJRKyV1ENxsn0rcEFEvJa2cyFwH8VlwwsjYu0wf0czMxsFzfZQ9umfliSKK62OHWKdsxoU3zxI/SuAKxqU3wvc20ycZmZWnWGPNhyFbwMntiAeMzPrUM0e8vpAaXYPivtS/r0lEZmZWUdq9iqvPy9Nb6U4/zErezRmZtaxmj2Hcl6rAzEzs87W7AO2DpJ0t6RN6XWnpINaHZyZmXWOZk/Kf43iXpED0+u7qczMzAxoPqF0R8TXImJrei0CulsYl5mZdZhmE8rzkj4oaUx6fRB4vpWBmZlZZ2k2oXwIOAP4JbABOB2Y06KYzMysAzV72fDngdkR8SKApP2BKykSjZmZWdM9lD/uTyYAEfECcHhrQjIzs07UbELZo/QwrP4eSrO9GzMz2w00mxS+DPxE0rfS/H+lwUCOZma2+2r2TvlbJa0A/jQVfSAi/Fx3MzPbpunDVimBOImYmVlDwx6+3szMrBEnFDMzy8IJxczMsnBCMTOzLJxQzMwsC9+c2EJT5t2z3Xzf/FMqisTMrPXcQzEzsyycUMzMLAsnFDMzy8IJxczMsnBCMTOzLJxQzMwsC1823ED95b5mZjY091DMzCyLliUUSQslbZK0plS2v6Slkp5KP/dL5ZJ0jaR1kh6VdERpndmp/lOSZrcqXjMz2zmt7KEsAmbWlc0DlkXEVGBZmgc4CZiaXnOB62Hbo4YvBY4BjgYuLT+K2MzM2kfLEkpEPAC8UFc8C7glTd8CnFoqvzUKDwLjJU0ETgSWRsQLEfEisJQdk5SZmbUBRUTrNi5NAZZExCFp/tcRMT5NC3gxIsZLWgLMj4gfpWXLgEuAGrB3RHwhlX8O+G1EXNlgX3Mpejd0d3cf2dPTw+r1m1v2u9U7dNK4Hcrq919fZ8uWLXR1dbU0rhwcZ16OM69OiLMTYgSYMWPGyog4aqTrV3aVV0SEpGzZLCIWAAsApk2bFrVajTmjeLVW39m1Hcrq919fp7e3l1ptx/XajePMy3Hm1QlxdkKMOYz2VV4b06Es0s9NqXw9MLlU76BUNlC5mZm1mdFOKIuB/iu1ZgPfKZWfm672OhbYHBEbgPuAEyTtl07Gn5DKzMyszbTskJek2ynOgRwg6TmKq7XmAz2SzgeeBc5I1e8FTgbWAb8BzgOIiBckXQ4sT/U+HxH1J/rNzKwNtCyhRMRZAyx6T4O6AVwwwHYWAgszhmZmZi3gO+XNzCwLj+WVicf/MrPdnRPKKKpPOhcfupVaNaGYmWXnQ15mZpaFE4qZmWXhhGJmZlk4oZiZWRZOKGZmloUTipmZZeGEYmZmWTihmJlZFk4oZmaWhROKmZll4YRiZmZZOKGYmVkWHhyyzTQatbhv/ikVRGJmNjzuoZiZWRZOKGZmloUTipmZZeGEYmZmWTihmJlZFk4oZmaWhROKmZll4YRiZmZZOKGYmVkWTihmZpaFE4qZmWXhhGJmZllUklAk9UlaLWmVpBWpbH9JSyU9lX7ul8ol6RpJ6yQ9KumIKmI2M7PBVdlDmRERh0XEUWl+HrAsIqYCy9I8wEnA1PSaC1w/6pGamdmQ2mn4+llALU3fAvQCl6TyWyMigAcljZc0MSI2VBJlBeqHtPdw9mbWjlR8T4/yTqVngBeBAP53RCyQ9OuIGJ+WC3gxIsZLWgLMj4gfpWXLgEsiYkXdNudS9GDo7u4+sqenh9XrN4/ibzV8E94Ev7//uO3Kmon50EnjhqyT05YtW+jq6hrVfY6E48zLcebTCTECzJgxY2XpqNGwVdVDOS4i1kv6fWCppCfKCyMiJA0r00XEAmABwLRp06JWqzGnwcOq2snFh27lb7//Sl3p0G9J39m1lsQzkN7eXmq10d3nSDjOvBxnPp0QYw6VnEOJiPXp5ybgbuBoYKOkiQDp56ZUfT0wubT6QanMzMzayKgnFEljJe3TPw2cAKwBFgOzU7XZwHfS9GLg3HS117HA5t3p/ImZWaeo4pDXBODu4jQJewLfiIjvS1oO9Eg6H3gWOCPVvxc4GVgH/AY4b/RDNjOzoYx6QomIp4E/aVD+PPCeBuUBXDAKoZmZ2U5op8uGrUn1lxGDLyU2s+p56BUzM8vCCcXMzLJwQjEzsyx8DmUX5fMsZjba3EMxM7MsnFDMzCwLJxQzM8vCCcXMzLJwQjEzsyycUMzMLAtfNrwb8ZMfzayV3EMxM7MsnFDMzCwLH/LaRTS6Mz7HdnxYzMya5R6KmZll4YRiZmZZOKGYmVkWPodig5oy7x4uPnQrc0rnVnxexcwacULZjeU6kW9mBk4olomvDjMzn0MxM7Ms3EOxYWvmUJmfGGm2+3EPxczMsnAPxUZNM+dZfC7GrHO5h2JmZlm4h2JtzedizDqHE4pVJveAluUbMJ10zEafE4rtkny+xmz0dUxCkTQTuBoYA9wUEfMrDskq0qo7/JvZrpOO2cA6IqFIGgNcB7wPeA5YLmlxRDxWbWTWKVr1vJhm1SeiVp4bGknvLOf+bffVEQkFOBpYFxFPA0i6A5gFOKFYRyh/gV986FYa/elV2fNqVK9+UNBmNZM8h1qn0XpOeO1PEVF1DEOSdDowMyI+nObPAY6JiAtLdeYCc9PsIcCaUQ90+A4AflV1EE1wnHk5zrw6Ic5OiBFgWkTsM9KVO6WHMqSIWAAsAJC0IiKOqjikITnOvBxnXo4zn06IEYo4d2b9TrmxcT0wuTR/UCozM7M20SkJZTkwVdLBkt4AnAksrjgmMzMr6YhDXhGxVdKFwH0Ulw0vjIi1g6yyYHQi22mOMy/HmZfjzKcTYoSdjLMjTsqbmVn765RDXmZm1uacUMzMLItdLqFIminpSUnrJM2rOp5+kiZLul/SY5LWSvpEKr9M0npJq9Lr5DaItU/S6hTPilS2v6Slkp5KP/erML5ppfZaJeklSRe1S1tKWihpk6Q1pbKG7afCNenz+qikIyqM8UuSnkhx3C1pfCqfIum3pXa9YTRiHCTOAd9nSZ9ObfmkpBMrjvObpRj7JK1K5VW250DfQ3k+nxGxy7woTtj/HHgr8AbgEWB61XGl2CYCR6TpfYB/AaYDlwF/U3V8dbH2AQfUlX0RmJem5wH/UHWcpff8l8AftEtbAscDRwBrhmo/4GTge4CAY4GfVhjjCcCeafofSjFOKddrg7Zs+D6nv6dHgDcCB6fvgjFVxVm3/MvA/2iD9hzoeyjL53NX66FsG6IlIv4D6B+ipXIRsSEiHkrTLwOPA5OqjWpYZgG3pOlbgFMrjKXsPcDPI+LZqgPpFxEPAC/UFQ/UfrOAW6PwIDBe0sQqYoyIH0TE1jT7IMX9XpUaoC0HMgu4IyJejYhngHUU3wktN1ickgScAdw+GrEMZpDvoSyfz10toUwCflGaf442/NKWNAU4HPhpKrowdScXVnkoqSSAH0haqWJIG4AJEbEhTf8SmFBNaDs4k+3/UNutLfsN1H7t+pn9EMV/pv0OlvSwpH+S9O6qgipp9D63a1u+G9gYEU+Vyipvz7rvoSyfz10tobQ9SV3AncBFEfEScD3wNuAwYANF17hqx0XEEcBJwAWSji8vjKIvXPn15ipucn0/8K1U1I5tuYN2ab+BSPossBW4LRVtAN4SEYcDnwK+IWnfquKjQ97nkrPY/p+eytuzwffQNjvz+dzVEkpbD9EiaS+KN/G2iLgLICI2RsRrEfE74EZGqYs+mIhYn35uAu6miGljf1c3/dxUXYTbnAQ8FBEboT3bsmSg9murz6ykOcCfAWenLxbSIaTn0/RKinMT76gqxkHe57ZqSwBJewIfAL7ZX1Z1ezb6HiLT53NXSyhtO0RLOo56M/B4RFxVKi8fjzyNikdJljRW0j790xQnatdQtOPsVG028J1qItzOdv/5tVtb1hmo/RYD56araY4FNpcOPYwqFQ+x+1vg/RHxm1J5t4pnEiHprcBU4OkqYkwxDPQ+LwbOlPRGSQdTxPmz0Y6vznuBJyLiuf6CKttzoO8hcn0+q7jSoJUviqsS/oUi63+26nhKcR1H0Y18FFiVXicD/wisTuWLgYkVx/lWiitlHgHW9rch8HvAMuAp4IfA/hXHORZ4HhhXKmuLtqRIchuA/6Q45nz+QO1HcfXMdenzuho4qsIY11EcL+//fN6Q6v5F+iysAh4C/rzithzwfQY+m9rySeCkKuNM5YuAj9TVrbI9B/oeyvL59NArZmaWxa52yMvMzCrihGJmZlk4oZiZWRZOKGZmloUTipmZZeGEYllJ6pV01Cjs5+OSHpd029C1h9zWRySdmyOuQfbxeUnvTdMXSXpzK/dXt++bJE0fRv2apHe2MqbhkjRe0n8vzR8o6f9UGZPtyJcNW1aSeilGgl0xgnX3jNcHJxyq7hPAe6N0w1inkNRHcT3/r6qOpRFJlwFbIuLKBsuafo8yxzQFWBIRh4z2vq157qHshtLzGB6XdGN6JsIPJL0pLdvWw5B0QPryQ9IcSd9Oz0rok3ShpE+lAe4elLR/aRfnqHjOwxpJR6f1x6aB/H6W1plV2u5iSf+X4saq+lg/lbazRtJFqewGihswvyfpk3X150i6tjS/RFItTW+RdIWkR1LME1L5ZZL+Jk0fmZY/ouL5IGua2O4Jkn4i6SFJ31IxTlL977FI0umSPg4cCNwv6f7B1k/t/PepLVdIOkLSfZJ+Lukjqc5ESQ+U2nuHgQbr3tOGbVD+bAAfAT6ZtvnuFPsNkn4KfFHS0SnehyX9P0nTSm10l6Tvq3iuxhdT+Zi0jTUqnrPzyVT+V5KWp1juVOq1SZqg4nks/e/DO4H5wNtSTF9Kn+H+92ZvSV9L235Y0ozB4rEWGq07NP1qnxfF8xi2Aoel+R7gg2m6l3Q3LHAA0Jem51DcSb0P0A1sJt0BDHyFYpC5/vVvTNPHk577APzP0j7GU4xmMDZt9zka3HkPHElxd+5YoIvi7uLD07I+6p7ZUorz2tL8EqCWpoN0VzLF8x/+Lk1fRnq+BsUdxMen6S+V4m+43dRGDwBjU/klpOde1MW1CDi9PvbB1k/1Plpq40dL7b8xlV/M66MZjAH2abDv8nvasA3q6m9rj1LsS0jPFgH25fXnprwXuLPURk8D44C9gWcpxoE6Elha2t749PP3SmVfAD6Wpr/J65+nMWl7U9j+mSjb5lMbLEzTfwj8a9p/w3iq/vvblV97YrurZyJiVZpeSfEHOpT7o3iGwsuSNgPfTeWrgT8u1bsdimdESNpXxZP/TgDe398ToPgDf0uaXhoRjZ4lcRxwd0S8AiDpLoqhwB9u5hds4D8ovhih+J3fV16Y4hwfxbMtoBji46QhtnksxQOKfiwJige7/WQYMQ21fv9YdKuBrlL7v5riXQ4sVDHg37dL7+lABm2DQXwrIl5L0+OAWyRNpUhQe5XqLYuIzQCSHqN48Nla4K2SvgrcA/wg1T1E0hco/sHoAu5L5X8KnAuQ9rlZgz+K4Djgq6n+E5Ke5fXBFhvF84uGW7Gd5oSy+3q1NP0a8KY0vZXXD4XuPcg6vyvN/47tP0v1J+aCYkygv4iIJ8sLJB0DvDKsyAdXjh+2/x3+M9K/sRS/83A+/wNtVxQJ8azhBtrk+uU2rm//PVPSPh44BVgk6aqIuHWQ/Y20Dcrv0eUU/1yclg6R9TaId9v2I+JFSX8CnEhxOO0MiuetLAJOjYhHVIxyXGsyluHYIZ4W7MMSn0Oxen0UhygATh/hNv4SQNJxFKOTbqb47/NjSv+GSzq8ie38M3CqpDerGPn4tFQ2mD7gMEl7SJrMMIawj4hfA79OcQOc3cR2HwTeJentsO1c0VBDkb9McehqpOtvI+kPKA5/3QjcRPEY2p1Vjq+Rcbw+hPmcoTYm6QBgj4i4E/g7Xo9xH2BD6l2V23oZ8NG07hhJ44aI6Z/7109t9xaKwSFtlDmhWL0rgY9Kepji+P5I/Hta/waK0WGh+K92L+BRSWvT/KCieFTpIoohyH8K3BQRQx3u+jHwDPAYcA3FaK7DcR5wnaRVFL2HQbcbEf9G8aV6u6RHKQ5X/eEQ+1gAfF/S/SNcv6wGPJLa+y+Bq4ex7kC+C5zWf1K+wfIvAn+f9tnMf/yTgN7Upl8HPp3KP0fxvv4YeKJU/xPADEmrKQ7LTY/i+SE/Tif2v1S3/f8F7JHqfxOYExGvYqPOlw2bDUC+VNVsWNxDMTOzLNxDMTOzLNxDMTOzLJxQzMwsCycUMzPLwgnFzMyycEIxM7Ms/j/oauPA9kIRSAAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# This is a code cell\n",
    "# --- Your code for generating the histogram goes here ---\n",
    "tmp = data2.groupby(\"InvoiceNo\")[\"StockCode\"].nunique().values\n",
    "\n",
    "plt.hist(tmp, bins = 200)\n",
    "plt.xlabel(\"number of unique items in transaction\")\n",
    "plt.ylabel(\"count\")\n",
    "plt.xlim(0, 200)\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The distribution is similar to exponential distribution. It showed that most of the people tend to buy a little number of unique items once.\n",
    "1. The count decreases dramatically when the number of unique items in transaction increases at first.\n",
    "2. The count turns to small decline subsequently even though the number of unique items in transaction is still inclining.\n",
    "3. Perhaps most people do not need so many different things in their daily lifes (only need to buy what they lack)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Section 4 - Mining Association Rules"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:blue\"> 1) Will we complete the counting before the sun burns out (the sun has another $ 5 \\times 10^9 < 2^{33} $ years to burn)? </span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<em>No</em>. The reason is as below:\n",
    "1. The total number of all possible itemsets would be approximate $3^n$, where $n$ is the number of the item. In this case, $n=3665$.\n",
    "2. We need about $\\frac{3^{3665}*2}{2^{36}}>2^{3630}$ seconds to consider compute the support and confidence for each rule.\n",
    "3. It is obviously that the needed time is much larger than $2^{33}*60*60*24*265$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is a code cell\n",
    "# --- Your code for preprocessing transactions goes here ---\n",
    "transactions = data2.groupby(\"InvoiceNo\")[\"StockCode\"].apply(list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:blue\"> 2) Run efficient-apriori in python with min_support=0.025, min_confidence=0.2, max_length=4. Write down the number of rules found and the rule with the highest lift. </span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time Usage: 17.577211\n",
      "22\n",
      "{85099B} -> {23203} (conf: 0.292, supp: 0.025, lift: 5.020, conv: 1.331)\n",
      "{23203} -> {85099B} (conf: 0.433, supp: 0.025, lift: 5.020, conv: 1.612)\n",
      "{22382} -> {20725} (conf: 0.473, supp: 0.025, lift: 6.797, conv: 1.764)\n",
      "{20725} -> {22382} (conf: 0.362, supp: 0.025, lift: 6.797, conv: 1.485)\n",
      "{20727} -> {20725} (conf: 0.491, supp: 0.028, lift: 7.067, conv: 1.830)\n",
      "{20725} -> {20727} (conf: 0.401, supp: 0.028, lift: 7.067, conv: 1.575)\n",
      "{22383} -> {20725} (conf: 0.498, supp: 0.028, lift: 7.156, conv: 1.852)\n",
      "{20725} -> {22383} (conf: 0.403, supp: 0.028, lift: 7.156, conv: 1.580)\n",
      "{85099B} -> {22386} (conf: 0.341, supp: 0.029, lift: 7.262, conv: 1.447)\n",
      "{22386} -> {85099B} (conf: 0.627, supp: 0.029, lift: 7.262, conv: 2.449)\n",
      "{22383} -> {20727} (conf: 0.446, supp: 0.025, lift: 7.855, conv: 1.702)\n",
      "{20727} -> {22383} (conf: 0.442, supp: 0.025, lift: 7.855, conv: 1.691)\n",
      "{22384} -> {20725} (conf: 0.562, supp: 0.028, lift: 8.078, conv: 2.123)\n",
      "{20725} -> {22384} (conf: 0.406, supp: 0.028, lift: 8.078, conv: 1.598)\n",
      "{22384} -> {20727} (conf: 0.498, supp: 0.025, lift: 8.781, conv: 1.880)\n",
      "{20727} -> {22384} (conf: 0.441, supp: 0.025, lift: 8.781, conv: 1.699)\n",
      "{82494L} -> {82482} (conf: 0.577, supp: 0.025, lift: 12.211, conv: 2.253)\n",
      "{82482} -> {82494L} (conf: 0.534, supp: 0.025, lift: 12.211, conv: 2.053)\n",
      "{22727} -> {22726} (conf: 0.604, supp: 0.029, lift: 14.198, conv: 2.420)\n",
      "{22726} -> {22727} (conf: 0.672, supp: 0.029, lift: 14.198, conv: 2.902)\n",
      "{22699} -> {22697} (conf: 0.691, supp: 0.029, lift: 18.534, conv: 3.115)\n",
      "{22697} -> {22699} (conf: 0.783, supp: 0.029, lift: 18.534, conv: 4.412)\n"
     ]
    }
   ],
   "source": [
    "# This is a code cell\n",
    "# --- Your code for apriori algorithm goes here ---\n",
    "beg = time.time()\n",
    "itemsets, rules = apriori(list(transactions.values), min_support = 0.025, min_confidence = 0.2, max_length = 4)\n",
    "end = time.time()\n",
    "\n",
    "print (\"Time Usage: {:.6f}\".format(end - beg))\n",
    "print (len(rules))\n",
    "for rule in sorted(rules, key = lambda rule: rule.lift):\n",
    "    print (rule)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The number of rules: 22<br>\n",
    "The rule with the highest lift:\n",
    "* {22699} -> {22697} (conf: 0.691, supp: 0.029, lift: 18.534, conv: 3.115)\n",
    "* {22697} -> {22699} (conf: 0.783, supp: 0.029, lift: 18.534, conv: 4.412)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:blue\"> 3) Run efficient-apriori in python with min_support=0.02, min_confidence=0.2, max_length=4. Write down the number of rules found and the rule with the highest lift. </span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time Usage: 46.565479\n",
      "88\n",
      "{85099B} -> {20725} (conf: 0.266, supp: 0.023, lift: 3.820, conv: 1.267)\n",
      "{20725} -> {85099B} (conf: 0.330, supp: 0.023, lift: 3.820, conv: 1.363)\n",
      "{85099B} -> {23203} (conf: 0.292, supp: 0.025, lift: 5.020, conv: 1.331)\n",
      "{23203} -> {85099B} (conf: 0.433, supp: 0.025, lift: 5.020, conv: 1.612)\n",
      "{47566} -> {23298} (conf: 0.282, supp: 0.021, lift: 5.178, conv: 1.317)\n",
      "{23298} -> {47566} (conf: 0.386, supp: 0.021, lift: 5.178, conv: 1.506)\n",
      "{85099B} -> {22411} (conf: 0.247, supp: 0.021, lift: 5.807, conv: 1.272)\n",
      "{22411} -> {85099B} (conf: 0.501, supp: 0.021, lift: 5.807, conv: 1.832)\n",
      "{22699} -> {22423} (conf: 0.536, supp: 0.023, lift: 5.835, conv: 1.959)\n",
      "{22423} -> {22699} (conf: 0.246, supp: 0.023, lift: 5.835, conv: 1.271)\n",
      "{22697} -> {22423} (conf: 0.541, supp: 0.020, lift: 5.888, conv: 1.979)\n",
      "{22423} -> {22697} (conf: 0.219, supp: 0.020, lift: 5.888, conv: 1.233)\n",
      "{23209} -> {20725} (conf: 0.417, supp: 0.023, lift: 5.989, conv: 1.595)\n",
      "{20725} -> {23209} (conf: 0.325, supp: 0.023, lift: 5.989, conv: 1.401)\n",
      "{85123A} -> {21733} (conf: 0.231, supp: 0.025, lift: 6.307, conv: 1.253)\n",
      "{21733} -> {85123A} (conf: 0.673, supp: 0.025, lift: 6.307, conv: 2.732)\n",
      "{23206} -> {20725} (conf: 0.449, supp: 0.021, lift: 6.459, conv: 1.689)\n",
      "{20725} -> {23206} (conf: 0.302, supp: 0.021, lift: 6.459, conv: 1.365)\n",
      "{85099B} -> {21931} (conf: 0.271, supp: 0.023, lift: 6.496, conv: 1.315)\n",
      "{21931} -> {85099B} (conf: 0.561, supp: 0.023, lift: 6.496, conv: 2.080)\n",
      "{22382} -> {20725} (conf: 0.473, supp: 0.025, lift: 6.797, conv: 1.764)\n",
      "{20725} -> {22382} (conf: 0.362, supp: 0.025, lift: 6.797, conv: 1.485)\n",
      "{20728} -> {20725} (conf: 0.475, supp: 0.025, lift: 6.833, conv: 1.773)\n",
      "{20725} -> {20728} (conf: 0.356, supp: 0.025, lift: 6.833, conv: 1.472)\n",
      "{20727} -> {20725} (conf: 0.491, supp: 0.028, lift: 7.067, conv: 1.830)\n",
      "{20725} -> {20727} (conf: 0.401, supp: 0.028, lift: 7.067, conv: 1.575)\n",
      "{22383} -> {20725} (conf: 0.498, supp: 0.028, lift: 7.156, conv: 1.852)\n",
      "{20725} -> {22383} (conf: 0.403, supp: 0.028, lift: 7.156, conv: 1.580)\n",
      "{85099B} -> {22386} (conf: 0.341, supp: 0.029, lift: 7.262, conv: 1.447)\n",
      "{22386} -> {85099B} (conf: 0.627, supp: 0.029, lift: 7.262, conv: 2.449)\n",
      "{85099F} -> {85099B} (conf: 0.633, supp: 0.022, lift: 7.334, conv: 2.490)\n",
      "{85099B} -> {85099F} (conf: 0.259, supp: 0.022, lift: 7.334, conv: 1.301)\n",
      "{22382} -> {20727} (conf: 0.423, supp: 0.023, lift: 7.455, conv: 1.635)\n",
      "{20727} -> {22382} (conf: 0.397, supp: 0.023, lift: 7.455, conv: 1.571)\n",
      "{23209} -> {23203} (conf: 0.440, supp: 0.024, lift: 7.558, conv: 1.683)\n",
      "{23203} -> {23209} (conf: 0.410, supp: 0.024, lift: 7.558, conv: 1.603)\n",
      "{20726} -> {20725} (conf: 0.528, supp: 0.023, lift: 7.593, conv: 1.972)\n",
      "{20725} -> {20726} (conf: 0.336, supp: 0.023, lift: 7.593, conv: 1.439)\n",
      "{22382} -> {20728} (conf: 0.396, supp: 0.021, lift: 7.594, conv: 1.569)\n",
      "{20728} -> {22382} (conf: 0.405, supp: 0.021, lift: 7.594, conv: 1.590)\n",
      "{22384} -> {22383} (conf: 0.430, supp: 0.022, lift: 7.636, conv: 1.655)\n",
      "{22383} -> {22384} (conf: 0.384, supp: 0.022, lift: 7.636, conv: 1.541)\n",
      "{20728} -> {20727} (conf: 0.436, supp: 0.023, lift: 7.679, conv: 1.672)\n",
      "{20727} -> {20728} (conf: 0.400, supp: 0.023, lift: 7.679, conv: 1.580)\n",
      "{22383} -> {20727} (conf: 0.446, supp: 0.025, lift: 7.855, conv: 1.702)\n",
      "{20727} -> {22383} (conf: 0.442, supp: 0.025, lift: 7.855, conv: 1.691)\n",
      "{22383} -> {22382} (conf: 0.429, supp: 0.024, lift: 8.040, conv: 1.657)\n",
      "{22382} -> {22383} (conf: 0.452, supp: 0.024, lift: 8.040, conv: 1.723)\n",
      "{22384} -> {20725} (conf: 0.562, supp: 0.028, lift: 8.078, conv: 2.123)\n",
      "{20725} -> {22384} (conf: 0.406, supp: 0.028, lift: 8.078, conv: 1.598)\n",
      "{23209} -> {23206} (conf: 0.378, supp: 0.021, lift: 8.085, conv: 1.532)\n",
      "{23206} -> {23209} (conf: 0.439, supp: 0.021, lift: 8.085, conv: 1.685)\n",
      "{22383} -> {20728} (conf: 0.433, supp: 0.024, lift: 8.316, conv: 1.673)\n",
      "{20728} -> {22383} (conf: 0.468, supp: 0.024, lift: 8.316, conv: 1.774)\n",
      "{22384} -> {20727} (conf: 0.498, supp: 0.025, lift: 8.781, conv: 1.880)\n",
      "{20727} -> {22384} (conf: 0.441, supp: 0.025, lift: 8.781, conv: 1.699)\n",
      "{22384} -> {20728} (conf: 0.459, supp: 0.023, lift: 8.801, conv: 1.751)\n",
      "{20728} -> {22384} (conf: 0.442, supp: 0.023, lift: 8.801, conv: 1.702)\n",
      "{22382} -> {20726} (conf: 0.410, supp: 0.022, lift: 9.266, conv: 1.620)\n",
      "{20726} -> {22382} (conf: 0.494, supp: 0.022, lift: 9.266, conv: 1.871)\n",
      "{23203} -> {23202} (conf: 0.396, supp: 0.023, lift: 9.515, conv: 1.587)\n",
      "{23202} -> {23203} (conf: 0.554, supp: 0.023, lift: 9.515, conv: 2.113)\n",
      "{22470} -> {22469} (conf: 0.508, supp: 0.022, lift: 9.801, conv: 1.928)\n",
      "{22469} -> {22470} (conf: 0.424, supp: 0.022, lift: 9.801, conv: 1.660)\n",
      "{82494L} -> {82482} (conf: 0.577, supp: 0.025, lift: 12.211, conv: 2.253)\n",
      "{82482} -> {82494L} (conf: 0.534, supp: 0.025, lift: 12.211, conv: 2.053)\n",
      "{22910} -> {22086} (conf: 0.647, supp: 0.024, lift: 12.239, conv: 2.684)\n",
      "{22086} -> {22910} (conf: 0.460, supp: 0.024, lift: 12.239, conv: 1.783)\n",
      "{22728} -> {22727} (conf: 0.646, supp: 0.021, lift: 13.654, conv: 2.691)\n",
      "{22727} -> {22728} (conf: 0.452, supp: 0.021, lift: 13.654, conv: 1.763)\n",
      "{22727} -> {22726} (conf: 0.604, supp: 0.029, lift: 14.198, conv: 2.420)\n",
      "{22726} -> {22727} (conf: 0.672, supp: 0.029, lift: 14.198, conv: 2.902)\n",
      "{23301} -> {23300} (conf: 0.612, supp: 0.025, lift: 17.877, conv: 2.492)\n",
      "{23300} -> {23301} (conf: 0.729, supp: 0.025, lift: 17.877, conv: 3.541)\n",
      "{22630} -> {22629} (conf: 0.688, supp: 0.023, lift: 18.123, conv: 3.086)\n",
      "{22629} -> {22630} (conf: 0.602, supp: 0.023, lift: 18.123, conv: 2.431)\n",
      "{22699} -> {22697} (conf: 0.691, supp: 0.029, lift: 18.534, conv: 3.115)\n",
      "{22697} -> {22699} (conf: 0.783, supp: 0.029, lift: 18.534, conv: 4.412)\n",
      "{22699} -> {22698} (conf: 0.557, supp: 0.024, lift: 18.564, conv: 2.189)\n",
      "{22698} -> {22699} (conf: 0.784, supp: 0.024, lift: 18.564, conv: 4.438)\n",
      "{22697, 22698} -> {22699} (conf: 0.848, supp: 0.021, lift: 20.071, conv: 6.294)\n",
      "{22699} -> {22697, 22698} (conf: 0.498, supp: 0.021, lift: 20.071, conv: 1.943)\n",
      "{22698} -> {22697} (conf: 0.827, supp: 0.025, lift: 22.193, conv: 5.576)\n",
      "{22697} -> {22698} (conf: 0.666, supp: 0.025, lift: 22.193, conv: 2.902)\n",
      "{22698, 22699} -> {22697} (conf: 0.894, supp: 0.021, lift: 23.995, conv: 9.125)\n",
      "{22697} -> {22698, 22699} (conf: 0.564, supp: 0.021, lift: 23.995, conv: 2.242)\n",
      "{22697, 22699} -> {22698} (conf: 0.721, supp: 0.021, lift: 24.033, conv: 3.475)\n",
      "{22698} -> {22697, 22699} (conf: 0.701, supp: 0.021, lift: 24.033, conv: 3.252)\n"
     ]
    }
   ],
   "source": [
    "# This is a code cell\n",
    "# --- Your code for apriori algorithm goes here ---\n",
    "beg = time.time()\n",
    "itemsets, rules = apriori(list(transactions.values), min_support = 0.020, min_confidence = 0.2, max_length = 4)\n",
    "end = time.time()\n",
    "\n",
    "print (\"Time Usage: {:.6f}\".format(end - beg))\n",
    "print (len(rules))\n",
    "for rule in sorted(rules, key = lambda rule: rule.lift):\n",
    "    print (rule)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The number of rules: 88<br>\n",
    "The rule with the highest lift:\n",
    "* {22697, 22699} -> {22698} (conf: 0.721, supp: 0.021, lift: 24.033, conv: 3.475)\n",
    "* {22698} -> {22697, 22699} (conf: 0.701, supp: 0.021, lift: 24.033, conv: 3.252)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:blue\"> 4) Run efficient-apriori in python with min_support=0.025, min_confidence=0.4, max_length=4. Write down the number of rules found and the rule with the highest lift. </span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time Usage: 18.066964\n",
      "19\n",
      "{23203} -> {85099B} (conf: 0.433, supp: 0.025, lift: 5.020, conv: 1.612)\n",
      "{22382} -> {20725} (conf: 0.473, supp: 0.025, lift: 6.797, conv: 1.764)\n",
      "{20727} -> {20725} (conf: 0.491, supp: 0.028, lift: 7.067, conv: 1.830)\n",
      "{20725} -> {20727} (conf: 0.401, supp: 0.028, lift: 7.067, conv: 1.575)\n",
      "{22383} -> {20725} (conf: 0.498, supp: 0.028, lift: 7.156, conv: 1.852)\n",
      "{20725} -> {22383} (conf: 0.403, supp: 0.028, lift: 7.156, conv: 1.580)\n",
      "{22386} -> {85099B} (conf: 0.627, supp: 0.029, lift: 7.262, conv: 2.449)\n",
      "{22383} -> {20727} (conf: 0.446, supp: 0.025, lift: 7.855, conv: 1.702)\n",
      "{20727} -> {22383} (conf: 0.442, supp: 0.025, lift: 7.855, conv: 1.691)\n",
      "{22384} -> {20725} (conf: 0.562, supp: 0.028, lift: 8.078, conv: 2.123)\n",
      "{20725} -> {22384} (conf: 0.406, supp: 0.028, lift: 8.078, conv: 1.598)\n",
      "{22384} -> {20727} (conf: 0.498, supp: 0.025, lift: 8.781, conv: 1.880)\n",
      "{20727} -> {22384} (conf: 0.441, supp: 0.025, lift: 8.781, conv: 1.699)\n",
      "{82494L} -> {82482} (conf: 0.577, supp: 0.025, lift: 12.211, conv: 2.253)\n",
      "{82482} -> {82494L} (conf: 0.534, supp: 0.025, lift: 12.211, conv: 2.053)\n",
      "{22727} -> {22726} (conf: 0.604, supp: 0.029, lift: 14.198, conv: 2.420)\n",
      "{22726} -> {22727} (conf: 0.672, supp: 0.029, lift: 14.198, conv: 2.902)\n",
      "{22699} -> {22697} (conf: 0.691, supp: 0.029, lift: 18.534, conv: 3.115)\n",
      "{22697} -> {22699} (conf: 0.783, supp: 0.029, lift: 18.534, conv: 4.412)\n"
     ]
    }
   ],
   "source": [
    "# This is a code cell\n",
    "# --- Your code for apriori algorithm goes here ---\n",
    "beg = time.time()\n",
    "itemsets, rules = apriori(list(transactions.values), min_support = 0.025, min_confidence = 0.4, max_length = 4)\n",
    "end = time.time()\n",
    "\n",
    "print (\"Time Usage: {:.6f}\".format(end - beg))\n",
    "print (len(rules))\n",
    "for rule in sorted(rules, key = lambda rule: rule.lift):\n",
    "    print (rule)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The number of rules: 19<br>\n",
    "The rule with the highest lift:\n",
    "* {22699} -> {22697} (conf: 0.691, supp: 0.029, lift: 18.534, conv: 3.115)\n",
    "* {22697} -> {22699} (conf: 0.783, supp: 0.029, lift: 18.534, conv: 4.412)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:blue\"> 5) Compare the first two cases. How do they differ in the time taken for the algorithm to run, the number of rules found, and the lift of the highest lift rule? Briefly explain why each of these findings occur, based on the effects of changing min_support.  </span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The second case with the smaller ```min_support``` has more rules found. It took about 40 seconds, while the first case spent only 18 seconds. The reason is shown below:\n",
    "1. The support refers to the popularity of an itemset (the number of transactions with both itemsets / total number of transactions); which indicates that the supports of rules are fixed by the data we got.\n",
    "2. If we set the threshold higher, the number of the rules that are satisfied would decrease.\n",
    "3. It is intuitive that the more rules we consider, the more time we need to compute the results.\n",
    "\n",
    "In addition, the highest lift of the first case is about 18 wheras that of the second case is about 24. This is because that:\n",
    "1. lift = confidence / support\n",
    "2. We consider the itemsets with lower support, lead to the formula with smaller denominator.\n",
    "3. We got the maximum lift with larger result."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:blue\"> 6) Compare the first and third case. How do they differ in the number of rules found? Briefly explain why this finding occurs, based on the effects of changing min_confidence. </span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are more rules in the first case. The only difference condition is ```min_confidence```. The main reason is stated:\n",
    "1. The confidences refers to the likelihood that an itemset is also bought if the other is bought (the number of transactions with both itemsets / total number of transactions containing specific itemset); which points that the confidences between each itemsets are fixed.\n",
    "2. If we set the threshold higher, the number of the rules that are satisfied would decrease."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:blue\"> 7) Report the descriptions of the items associated with the highest lift rule you found in the three queries (you may use any method to do this). Does the rule make sense? </span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROSES REGENCY TEACUP AND SAUCER \n",
      "GREEN REGENCY TEACUP AND SAUCER\n",
      "PINK REGENCY TEACUP AND SAUCER\n"
     ]
    }
   ],
   "source": [
    "tmp = data2[[\"StockCode\", \"Description\"]].drop_duplicates()\n",
    "tmp.set_index(\"StockCode\", inplace=True)\n",
    "print (tmp.at[\"22699\", \"Description\"])\n",
    "print (tmp.at[\"22697\", \"Description\"])\n",
    "print (tmp.at[\"22698\", \"Description\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scenario 1:\n",
    "* {22699} -> {22697} (conf: 0.691, supp: 0.029, lift: 18.534, conv: 3.115)\n",
    "* {22697} -> {22699} (conf: 0.783, supp: 0.029, lift: 18.534, conv: 4.412)\n",
    "\n",
    "Scenario 2:\n",
    "* {22697, 22699} -> {22698} (conf: 0.721, supp: 0.021, lift: 24.033, conv: 3.475)\n",
    "* {22698} -> {22697, 22699} (conf: 0.701, supp: 0.021, lift: 24.033, conv: 3.252)\n",
    "\n",
    "Scenario 3:\n",
    "* {22699} -> {22697} (conf: 0.691, supp: 0.029, lift: 18.534, conv: 3.115)\n",
    "* {22697} -> {22699} (conf: 0.783, supp: 0.029, lift: 18.534, conv: 4.412)\n",
    "\n",
    "Descriptions:\n",
    "* 22697: ```GREEN REGENCY TEACUP AND SAUCER```\n",
    "* 22698: ```PINK  REGENCY TEACUP AND SAUCER```\n",
    "* 22699: ```ROSES REGENCY TEACUP AND SAUCER```\n",
    "\n",
    "I think it make sense cause:\n",
    "1. \"Regency teacup and saucer\" is a special category compared to other, such as clothes.\n",
    "2. People who like regency teacup and saucer usually like to collect them, and perhaps treat them in pairs as decoration.\n",
    "3. They might need various styles for different occasion or to match the personalities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
